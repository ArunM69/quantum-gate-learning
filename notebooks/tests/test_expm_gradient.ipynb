{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  47.30089773,   41.58751062,   11.79613877,   66.60876992],\n",
       "       [  78.81358454,   75.40439209,   24.08270378,  111.51496174],\n",
       "       [  39.3271675 ,   37.86378273,   12.24411345,   55.23865786],\n",
       "       [  84.22034043,   77.52048578,   23.29762595,  118.82826057]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[  47.35543075,   41.6371479 ,   11.8108558 ,   66.68567484],\n",
       "       [  78.9067194 ,   75.49150835,   24.10962867,  111.64639196],\n",
       "       [  39.373539  ,   37.90726408,   12.25764708,   55.30415484],\n",
       "       [  84.31863947,   77.61125404,   23.32517586,  118.96702555]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import scipy.linalg as la\n",
    "import theano\n",
    "import theano.tensor.slinalg\n",
    "import theano.tensor as T\n",
    "\n",
    "n = 4\n",
    "\n",
    "A = np.zeros(shape=(n, n), dtype=np.float)\n",
    "A += scipy.sparse.rand(4, 4, density=.8)\n",
    "A = np.asarray(A)\n",
    "\n",
    "\n",
    "# ----------- numerical differentiation\n",
    "x = 2.\n",
    "h = 0.001\n",
    "numerical_diff = (la.expm((x + h) * A) - la.expm(x * A)) / h\n",
    "\n",
    "# display gradient computed with numerical differentiation\n",
    "display(numerical_diff)\n",
    "\n",
    "\n",
    "\n",
    "# ----------- automatic differentiation with theano\n",
    "x = T.dscalar('x')\n",
    "expA = T.slinalg.expm(x * A)\n",
    "# the flattening is for more easily scan through the elements of the matrix\n",
    "# (theano.grad only accepts scalar cost)\n",
    "expA_flat = T.flatten(expA)\n",
    "\n",
    "def compute_element_grad(idx, flattened_matrix):\n",
    "    return T.grad(flattened_matrix[idx], wrt=x)\n",
    "# `theano.scan` basically loops over the elements of the matrix, and returns the gradient of each \n",
    "g_x_flat, _ = theano.scan(\n",
    "    fn=compute_element_grad,\n",
    "    sequences=T.arange(expA_flat.shape[0]),\n",
    "    non_sequences=[expA_flat]\n",
    ")\n",
    "# deflatten result\n",
    "g_x = T.reshape(g_x_flat, newshape=expA.shape)\n",
    "\n",
    "# here is where the computational graph is actually compiled\n",
    "gradient = theano.function(inputs=[x], outputs=g_x)\n",
    "\n",
    "\n",
    "# compute and display gradient computed with AD\n",
    "display(gradient(2.))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:theano]",
   "language": "python",
   "name": "conda-env-theano-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
